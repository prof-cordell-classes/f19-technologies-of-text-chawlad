# Lab Report: Text as Data

#### Dipshika Chawla

## Process Description

For this lab we worked in the online R Studio environment and learned about the various applications of code in humanities, specifically English or broadly speaking, Linguistics. We walked through an .rmd file that Prof Cordell prepared for us beforehand. This .rmd file was a useful resource as it guided us through the various snippets of code and allowed us to see the technical aspects and their interpretive significance simultaneously. We came across several basics and terminologies in R, and in programming in general, including packages, functions, variables, observations, datasets, etc. 

We experimented with elementary mathematical calculations and _print (“Hello World”)_, and built upon such ‘theory’ or foundational examples to help understand relatively complex applications of code in contextually relevant areas. In practice, we loaded the _tidyverse_ and _tidytext_ libraries, and the _gutenbergR_ package - all of which were pre-installed in our R Studio Server Application. We looked at blocks of code that served varying purposes such as extracting tokens and their frequencies from specific books in the Gutenberg Project. We manipulated elements of the code to see how that returned different results when we switch between nested/unnested tokens, include/exclude stop words, and such to optimise our code and analysis for its intended purpose. 

## Observations

I enjoyed browsing through this file’s combination of code and prose. Modern coding uses assembly languages, and these languages are becoming increasingly abstract with simple functions that contain various underlying instructions. Despite this attempt at bridging the gap between machine language and human language, the low levels of literacy in computational thinking and programming among the majority is a key obstacle in making coding widely accessible. The .rmd file type gives the file creator the perfect opportunity to weave together code and descriptions - maximising interpretation and engagement from the learner’s perspective. Despite having some experience with coding, I appreciate well-crafted prose or rationale for different instructions and snippets of code. I think this dual-language approach, technical and non-technical, is one that really helped me navigate and learn through the lab (and after, as a piece of reference).

The whole premise of coding as being a specific set of sequential instructions (i.e. algorithms) makes it very important for us to define instructions as precisely as possible. The system can only apply code within the bounds of its definition, which makes it the coder’s decision to place accurate bounds and maintain the optimal level of detail. The system can extract nested as well as unnested tokens, which makes it a crucial point of consideration for the coder. Such endless possibilities introduce questions such as ‘what method or instruction best complements my purpose and analysis?’ This has always intrigued me in terms of data and algorithms, as **the combination and extent of instructions is highly driven by purpose.** This idea was surfaced when we looked at Austen’s works, and the code returned a fairly invaluable insight of the jarringly high frequency of the word ’the’ as the code did not exclude the list of stop words. **Excluding the list of stop words** from our token extractions from Austen’s books (with the token size being 1 word) allowed the system to output a list of words that we perceive as carrying **more semantic meaning.** What is more semantic in one analysis differs from that in another analysis - making such **seemingly small manipulations and commands very valuable or assistive in analysis.** While in some cases such **literal and granular insight** might be relevant, it’s often not, and this can be incorporated in the code by discretion and expertise of the coder. Again, this expertise is subjective and highly influenced by the individual’s domain knowledge - for which one measure can be the individual’s familiarity with Zipf's Law if working with linguistic projects. Personally, I view this as a dominant affordance of modern coding - the idea of stacking meaningful building blocks to create an intricate filter or literal ’sieve’ for big data and information.

## Analysis

There are many parallels between our readings of the Analytical Engine in _The Thrilling Adventures of Lovelace and Babbage_ by Sydney Padua and our interactions with 21st century code in R for the ToT Lab 9. Firstly, I noticed very identifiable similarities simply between the various elements or ‘moving parts’ of the computing processes of the past and present. Both processes have distinct hardware, software, volatile (working) memory, inputs and outputs. Pauda’s illustration on page 90 calls out the parts of the Analytical Engine and although their literal forms and our ways of interacting with technology have evolved, these **elements continue to be of significant basis of how we structure and attribute the elements in computing technologies.** We witnessed such parallels in our lab, as variables in R are like the physical cards that Lovelace and Babbage imagined to use to temporarily save input for later stages of computation. Both share the key feature of volatility of data stored, and how it is at risk of being lost if the system is reset and working memory lost. The ‘stops’ or _shortcuts for common sequences of punches_ in the Analytical Engine are what 21st century assembly languages contain in a function - essentially _shortcuts for common sequences_ of code. The output is perhaps the easiest to comprehend as our current civilisations still rely on printed sheets of paper with computed information to some extent. However, we have seen a rise in our adoption of digital outputs and digital archives of outputs as well - all made possible due to computing technologies. It is impossible to envision the current scenario without being lured into the meta-spiral and omnipresence of Lovelace and Babbage’s early ideas and invention. 

The mention of AI and its affordances and limitations on pages 162-3 are particularly interesting because of the accuracy in early predictions and concerns raised by John Searle in his thought experiment, ‘The Chinese Room.’ I think the specific example of the Chinese language, and the early days of AI in the late 1900s, is the perfect combination to illustrate the threats that surround a system built on a
> a complete set of rote instructions on responding to a given set of Chinese characters, and a person ignorant of the Chinese language

This highlights the caution that needs to be accounted for when building an autonomous system and reminds me of one of the key principles in today’s realm of data processing - Garbage In Garbage Out (GIGO). Even though this principle is often mentioned when speaking of the quality of data, it is highly relevant when discussing the inputs and information used to program the software itself. This is also surfaced in Pauda’s footnote as she says,
> The Chinese Room replies by consulting instructions and feeding the resulting responses back through the slots

To sum that, if the ‘consulting instructions’ are flawed, the output is flawed not due to the processing capabilities of the machinery, but due to the inherent incompetence built into the software and its approach.

The Analytical Engine has undergone various iterations and interpretations over the decades, making possible our virtually robust and connected world today. Though the technology disappointed Queen Victoria at the time, as illustrated by Pauda on pages 75-7, this computational thinking by Babbage and Lovelace created a range of possibilities for people across disciplines. As said on page 78,
>The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform.

The idea of the Analytical Engine from the mid-19th century has scaled in infinite directions and ways as once imagined by Lovelace. In terms of the ’input’ - the Analytical Engine was restricted to numbers and mathematical calculations. It is also important to keep in mind the physicality and complexity of this hardware. Our inputs today range from all types and units of data - once normalised for readability by computers. Lovelace’s ability to envision how computers could perfectly intake orders and run operations as _ordered_ is what empowers all people to manipulate data, feed it into a computer system, and perform various tasks. This idea was at the core of our ‘Text as Data’ lab wherein we work with non-numeric input, non-binary instructions, and many versions of the same output (textual or visual.) 
